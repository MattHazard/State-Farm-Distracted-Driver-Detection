{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#yeeeeedsffdfddsfad\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "from imageio import imread\n",
    "#numpy.array(Image.fromarray(arr).resize()) for image resizing\n",
    "\n",
    "#image resolution\n",
    "\n",
    "#globals for rows and cols since we will always be doing images of the same size\n",
    "rows = 80\n",
    "cols = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "        \n",
    "def read_and_normalize_train_data():\n",
    "    cache_path = os.path.join('cache', 'train_r_' + str(80) + '_c_' + str(60) + '_t_' + str(3) + '.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target, driver_id, unique_drivers = load_train()\n",
    "        cache_data((train_data, train_target, driver_id, unique_drivers), cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target, driver_id, unique_drivers) = restore_data(cache_path)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "    train_data = train_data.reshape(train_data.shape[0], 3, 80, 60)\n",
    "    train_target = np_utils.to_categorical(train_target, 10)\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data /= 255\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, driver_id, unique_drivers\n",
    "\n",
    "\n",
    "def read_and_normalize_test_data():\n",
    "    cache_path = os.path.join('cache', 'test_r_' + str(80) + '_c_' + str(60) + '_t_' + str(3) + '.dat')\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_id = load_test()\n",
    "        cache_data((test_data, test_id), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.reshape(test_data.shape[0], 3, 80, 60)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id\n",
    "\n",
    "def get_driver_data():\n",
    "    dr = dict()\n",
    "    path = ('driver_imgs_list.csv')\n",
    "    #print(path)\n",
    "    print('Read drivers data')\n",
    "    f = open(path, 'r')\n",
    "    line = f.readline()\n",
    "    while (1):\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        arr = line.strip().split(',')\n",
    "        dr[arr[2]] = arr[0]\n",
    "    f.close()\n",
    "    return dr\n",
    "\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    driver_id = []\n",
    "    start_time = time.time()\n",
    "    driver_data = get_driver_data()\n",
    "\n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('train', 'c' + str(j), '*.jpg')\n",
    "        #print(path)\n",
    "        #return\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            img = cv2.imread(fl)\n",
    "            #img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "            driver_id.append(driver_data[os.path.basename(fl)])\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    unique_drivers = sorted(list(set(driver_id)))\n",
    "    print('Unique drivers: {}'.format(len(unique_drivers)))\n",
    "    #print(unique_drivers)\n",
    "    \n",
    "    return X_train, y_train, driver_id, unique_drivers\n",
    "\n",
    "def load_test():\n",
    "    print('Read test images')\n",
    "    start_time = time.time()\n",
    "    path = os.path.join('test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        img = cv2.imread(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(os.path.basename(fl))\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "    \n",
    "    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_test, X_test_id\n",
    "\n",
    "#testing\n",
    "\n",
    "def copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n",
    "    data = []\n",
    "    target = []\n",
    "    index = []\n",
    "    for i in range(len(driver_id)):\n",
    "        if driver_id[i] in driver_list:\n",
    "            data.append(train_data[i])\n",
    "            target.append(train_target[i])\n",
    "            index.append(i)\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    #print('hi2',data.shape)\n",
    "    target = np.array(target, dtype=np.float32)\n",
    "    index = np.array(index, dtype=np.uint32)\n",
    "    return data, target, index\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Convolution2D(32, 3, 3, border_mode='same', init='he_normal', input_shape=(1, 80, 60,)))\n",
    "    #input_shape=(1, 60, 80)\n",
    "    model.add(Convolution2D(32, (3, 3), input_shape=(3, rows, cols), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D(pool_size=(8, 8), data_format='channels_first'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "Read train images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n",
      "Read train data time: 9.85 seconds\n",
      "Unique drivers: 26\n",
      "Directory doesnt exists\n",
      "Train shape: (22424, 3, 80, 60)\n",
      "22424 train samples\n",
      "Read test images\n",
      "Read 7972 images from 79726\n",
      "Read 15944 images from 79726\n",
      "Read 23916 images from 79726\n",
      "Read 31888 images from 79726\n",
      "Read 39860 images from 79726\n",
      "Read 47832 images from 79726\n",
      "Read 55804 images from 79726\n",
      "Read 63776 images from 79726\n",
      "Read 71748 images from 79726\n",
      "Read 79720 images from 79726\n",
      "Read test data time: 32.57 seconds\n",
      "Directory doesnt exists\n",
      "Test shape: (79726, 3, 80, 60)\n",
      "79726 test samples\n"
     ]
    }
   ],
   "source": [
    "train_data, train_target, driver_id, unique_drivers = read_and_normalize_train_data()\n",
    "test_data, test_id = read_and_normalize_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Single Run\n",
      "Split train:  21601 21601\n",
      "Split valid:  823 823\n",
      "Train drivers:  ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024', 'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072', 'p075']\n",
      "Test drivers:  ['p081']\n",
      "(21601, 3, 80, 60) (21601, 10)\n",
      "(823, 3, 80, 60) (823, 10)\n",
      "Train on 21601 samples, validate on 823 samples\n",
      "Epoch 1/1\n",
      "21601/21601 [==============================] - 12s 562us/step - loss: 2.5972 - accuracy: 0.1035 - val_loss: 2.2899 - val_accuracy: 0.1640\n",
      "823/823 [==============================] - 0s 156us/step\n",
      "Score log_loss:  2.289948450113994\n",
      "79726/79726 [==============================] - 10s 126us/step\n",
      "Final log_loss: 2.289948450113994, rows: 80 cols: 60 epochs: 1\n"
     ]
    }
   ],
   "source": [
    "#d = get_driver_data()\n",
    "#x,y,z,c = load_train()\n",
    "#x,y = load_test()\n",
    "#cv2.imshow('img',x[0])\n",
    "#print(y)\n",
    "#print(z)\n",
    "#print(c)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 3, 80, 60)\n",
    "#Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "\n",
    "\n",
    "yfull_train = dict()\n",
    "yfull_test = []\n",
    "\n",
    "\n",
    "unique_list_train = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024',\n",
    "                     'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049',\n",
    "                     'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072',\n",
    "                     'p075']\n",
    "X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n",
    "unique_list_valid = ['p081']\n",
    "X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n",
    "\n",
    "print('Start Single Run')\n",
    "print('Split train: ', len(X_train), len(Y_train))\n",
    "print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "print('Train drivers: ', unique_list_train)\n",
    "print('Test drivers: ', unique_list_valid)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "              verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "# score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "# print('Score log_loss: ', score[0])\n",
    "\n",
    "predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "score = log_loss(Y_valid, predictions_valid)\n",
    "print('Score log_loss: ', score)\n",
    "\n",
    "# Store valid predictions\n",
    "for i in range(len(test_index)):\n",
    "    yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "# Store test predictions\n",
    "test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "yfull_test.append(test_prediction)\n",
    "\n",
    "print('Final log_loss: {}, rows: {} cols: {} epochs: {}'.format(score, rows, cols, epochs))\n",
    "info_string = 'loss_' + str(score) \\\n",
    "                + '_r_' + str(rows) \\\n",
    "                + '_c_' + str(cols) \\\n",
    "                + '_ep_' + str(epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (State-Farm-Distracted-Driver-Detection)",
   "language": "python",
   "name": "pycharm-9cfc4656"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
